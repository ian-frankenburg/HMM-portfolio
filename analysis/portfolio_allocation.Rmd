---
title: "A Bayesian Hidden Markov Model Approach to Resampled Frontiers"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{Rcpp gibbs, include=F, echo=F, cache=T}
#define ARMA_NO_DEBUG
#include <RcppDist.h>
#include <RcppThread.h>
// [[Rcpp::depends(RcppArmadillo, RcppDist, RcppThread)]]

using namespace arma;

// forward filter portion of FFBS algorithm
// this function computes the Gaussian likelihood for each regime by treating the
// Markovian dynamical system as a latent (or augmented) state
void forward(mat& filter, mat& predict, mat& likelihood, vec& marginal,
             const mat& y, const mat& mu, const cube& Sigma, const mat& pr, const double init0)
{
  int T = y.n_rows;
  double p00 = pr(0,0);
  double p11 = pr(1,1);
  double init1 = 1 - init0;
  double p01 = 1 - p00; 
  double p10 = 1 - p11;
  
  for (int t=0; t<T; t++){
    if(t==0){
      likelihood(t,0) = dmvnorm(y.row(t), mu.col(0), Sigma.slice(0))(0);
      likelihood(t,1) = dmvnorm(y.row(t), mu.col(1), Sigma.slice(1))(0);
      
      marginal(t) = p00 * likelihood(t,0) * init0 +
        p10*likelihood(t,0) * init1 +
        p01*likelihood(t,1) * init0 +
        p11*likelihood(t,1) * init1;
      
      predict(t,0) = p00 * init0 + p10 * init1;
      predict(t,1) = p01 * init0 + p11 * init1;
      
      filter(t,0) = likelihood(t,0) * predict(t,0) / marginal(t);
      filter(t,1) = likelihood(t,1) * predict(t,1) / marginal(t);
    }else{
      likelihood(t,0) = dmvnorm(y.row(t), mu.col(0), Sigma.slice(0), 0)(0);
      likelihood(t,1) = dmvnorm(y.row(t), mu.col(1), Sigma.slice(1), 0)(0);
      
      marginal(t) = p00 * likelihood(t,0) * filter(t-1,0) +
        p10*likelihood(t,0) * filter(t-1,1) +
        p01*likelihood(t,1) * filter(t-1,0) +
        p11*likelihood(t,1) * filter(t-1,1);
      
      predict(t,0) = p00 * filter(t-1,0) + p10 * filter(t-1,1);
      predict(t,1) = p01 * filter(t-1,0) + p11 * filter(t-1,1);
      
      filter(t,0) = likelihood(t,0) * predict(t,0) / marginal(t);
      filter(t,1) = likelihood(t,1) * predict(t,1) / marginal(t);
    }
  }
}

// backward sample portion of FFBS
// a stochastic version of backward smoothing in time
void backward(const mat& filter, const mat& pr, vec& s, vec& n)
{
  int T = filter.n_rows; double pr0;
  n.zeros(4);
  pr0 = filter(T-1, 0);
  s(T-1) = (R::runif(0, 1) < pr0) ? 0 : 1;;
  (s(T-1)==0) ? n(0)++: n(3)++;
  for(int t = (T-2); t >= 0; t--){
    pr0 = pr(0,s(t+1))*filter(t, 0) / dot(pr.col(s(t+1)), filter.row(t));
    s(t) = (R::runif(0,1)<pr0) ? 0 : 1;
    if(s(t+1)==0 && s(t)==0){
      n(0)++;
    }else if(s(t+1)==1 && s(t)==0){
      n(1)++;
    }else if(s(t+1)==0 && s(t)==1){
      n(2)++;
    }else if(s(t+1)==1 && s(t)==1){
      n(3)++;
    }
  }
}

// compute the transition probabilities, conditional on a realized backwards sample
void transition(const int& n00, 
                const int& n10, const int& n01, const int& n11, mat& pr)
{
  int u00 = 5, u01 = 1, u10 = 1, u11 = 5;
  double p = R::rbeta(u11 + n11, u10 + n10);
  double q = R::rbeta(u00 + n00, u01 + n01);
  pr(0,0) = q;
  pr(0,1) = 1-q;
  pr(1,0) = 1-p;
  pr(1,1) = p;
}

// helper function for sampling regime mean estimates
mat colMeans(mat y) 
{
  mat x(y.n_cols, 1, fill::zeros);
  for(uword i=0; i<y.n_cols; i++){
    x.row(i) = mean(y.col(i));
  }
  return(x);
}

// helper function for sampling regime mean estimates
mat colDiff(mat y, vec mu) 
{
  mat x(y.n_rows, y.n_cols, fill::zeros);
  for(uword i=0; i<y.n_cols; i++){
    x.col(i) = y.col(i)-mu(i);
  }
  return(x);
}

// draw a sample for location parameter
void location(const mat& y, const vec& s, const mat& mu0, mat&mu_draw,
              const cube& Sigma0, const cube& Sigma1)
{
  
  uword p = y.n_cols;
  vec nn(2, fill::zeros);
  mat mu1(p,2, fill::zeros);
  cube Lambda(p, p, 2, fill::zeros);
  
  nn(0) =  uvec(find(s==0)).n_elem;
  Lambda.slice(0)=inv(inv(Sigma0.slice(0))+nn(0)*inv(Sigma1.slice(0)));
  mu1.col(0) = Lambda.slice(0)*(inv(Sigma0.slice(0))*mu0.col(0) +
    nn(0)*inv(Sigma1.slice(0))*colMeans(y.rows(find(s==0))));
  
  nn(1) =  uvec(find(s==1)).n_elem;
  Lambda.slice(1)=inv(inv(Sigma0.slice(1))+nn(1)*inv(Sigma1.slice(1)));
  mu1.col(1) = Lambda.slice(1)*(inv(Sigma0.slice(1))*mu0.col(1) +
    nn(1)*inv(Sigma1.slice(1))*colMeans(y.rows(find(s==1))));
  
  mu_draw.col(0) = rmvnorm(1, mu1.col(0), Lambda.slice(0)).t();
  mu_draw.col(1) = rmvnorm(1, mu1.col(1), Lambda.slice(1)).t();
}

// sample covariance parameters
void scale(const mat& y, const vec& s, const vec& v0, const mat& mu, const cube& S0, cube& S)
{
  uword p = y.n_cols;
  mat S1_0(p,p,fill::zeros);
  uvec s0 = find(s==0);
  S1_0 = S0.slice(0) + colDiff(y.rows(s0), mu.col(0)).t()*colDiff(y.rows(s0), mu.col(0));
  
  mat S1_1(p,p,fill::zeros);
  uvec s1 = find(s==1);
  S1_1 = S0.slice(1) + colDiff(y.rows(s1), mu.col(1)).t()*colDiff(y.rows(s1), mu.col(1));
  
  mat W0 = inv(rwish(v0(0)+s0.n_elem, inv(S1_0)));
  mat W1 = inv(rwish(v0(1)+s1.n_elem, inv(S1_1)));
  
  S = join_slices(W0,W1);
}

// compute posterior predictive distribution forecast
cube posterior_predictive_draws(const int niter, const int burnin, const int h, 
                               const cube& pr_save, const mat& S_save,
                               const mat& mu0_save, const mat& mu1_save, 
                               const cube& Sigma0_save, const cube& Sigma1_save){
  int sample = 0, p = mu0_save.n_rows, T = S_save.n_rows;
  vec s_(h, fill::zeros);
  cube y_pred(h, p, niter, fill::zeros);
  mat w(p, niter, fill::zeros);
  for(int i=0; i<niter; i++){
    sample = (int) Rcpp::runif(1, 0, niter-burnin)(0);
    s_(0) = (S_save.col(sample)(T-1) == 0) ?
    R::rbinom(1, pr_save.slice(sample)(0,0)): R::rbinom(1, pr_save.slice(sample)(1,1));
    if(s_(0) == 0){
      y_pred.slice(i).row(0) = rmvnorm(1, mu0_save.col(sample), Sigma0_save.slice(sample));
      w.col(i) = inv(Sigma0_save.slice(sample)) * mu0_save.col(sample) / sum(inv(Sigma0_save.slice(sample)) * mu0_save.col(sample));
    }else{
      y_pred.slice(i).row(0) = rmvnorm(1, mu1_save.col(sample), Sigma1_save.slice(sample));
      w.col(i) = inv(Sigma1_save.slice(sample)) * mu1_save.col(sample) / sum(inv(Sigma1_save.slice(sample)) * mu1_save.col(sample));
    }
    for(int j=1; j<h; j++){
      s_(j) = (s_(j-1) == 0) ?
      R::rbinom(1, pr_save.slice(sample)(0,0)): R::rbinom(1, pr_save.slice(sample)(1,1));
      if(s_(j)==0){
        y_pred.slice(i).row(j) = rmvnorm(1, mu0_save.col(sample), Sigma0_save.slice(sample));
      }else{
        y_pred.slice(i).row(j) = rmvnorm(1, mu1_save.col(sample), Sigma1_save.slice(sample));
      }
    }
  }
  return y_pred;
}

// feed in a vector of m which contains the returns to construct the stochastic frontier
// use expressions on PG 384 of 'Bayesian Inference of State Space Models' book
// to get portfolio weights for each resampled frontier
mat porftfolio_weights(const int niter, const int burnin, const int h, 
                                const cube& pr_save, const mat& S_save,
                                const mat& mu0_save, const mat& mu1_save, 
                                const cube& Sigma0_save, const cube& Sigma1_save, 
                                const double nugget, const double m){
  int sample = 0, p = mu0_save.n_rows, T = S_save.n_rows;
  vec s_(h, fill::zeros);
  mat w(p, niter, fill::zeros), Sig_inv(p, p, fill::ones);
  vec one(p, fill::ones), mu(p, fill::zeros);
  for(int i=0; i<niter; i++){
    sample = (int) Rcpp::runif(1, 0, niter-burnin)(0);
    s_(0) = (S_save.col(sample)(T-1) == 0) ?
    R::rbinom(1, pr_save.slice(sample)(0,0)): R::rbinom(1, pr_save.slice(sample)(1,1));
    if(s_(0) == 0){
      Sig_inv = inv(Sigma0_save.slice(sample)+nugget*eye(p,p));
      mu = mu0_save.col(sample);
      // w.col(i) = inv(Sigma0_save.slice(sample)+nugget*eye(p,p)) * mu0_save.col(sample) / sum(inv(Sigma0_save.slice(sample)+nugget*eye(p,p)) * mu0_save.col(sample));
      w.col(i) = Sig_inv * (m*as_scalar(one.t()*Sig_inv*one)*mu
                              - as_scalar(mu.t()*Sig_inv*one)*mu
                              + as_scalar(mu.t()*Sig_inv*mu)*one
                              - m*as_scalar(mu.t()*Sig_inv*one)*one)
                              / (dot(mu,Sig_inv*mu)*dot(one, Sig_inv*one) - pow(dot(mu, Sig_inv*one),2));
        
    }else{
      // w.col(i) = inv(Sigma1_save.slice(sample)+nugget*eye(p,p)) * mu1_save.col(sample) / sum(inv(Sigma1_save.slice(sample)+nugget*eye(p,p)) * mu1_save.col(sample));
      Sig_inv = inv(Sigma1_save.slice(sample)+nugget*eye(p,p));
      mu = mu1_save.col(sample);
      w.col(i) = Sig_inv * (m*as_scalar(one.t()*Sig_inv*one)*mu
                              -as_scalar(mu.t()*Sig_inv*one)*mu
                              + as_scalar(mu.t()*Sig_inv*mu)*one
                              - m*as_scalar(mu.t()*Sig_inv*one)*one)
                              / (dot(mu,Sig_inv*mu)*dot(one, Sig_inv*one) - pow(dot(mu, Sig_inv*one),2));
    }
  }
  return w;
}

vec frontier(const int niter, const int burnin, const int h, 
                       const cube& pr_save, const mat& S_save,
                       const mat& mu0_save, const mat& mu1_save, 
                       const cube& Sigma0_save, const cube& Sigma1_save, 
                       const double nugget, const double m){
  int sample = 0, p = mu0_save.n_rows, T = S_save.n_rows;
  vec s_(h, fill::zeros);
  mat w(p, niter, fill::zeros), Sig_inv(p, p, fill::ones);
  vec one(p, fill::ones), mu(p, fill::zeros), risk(niter, fill::zeros);
  for(int i=0; i<niter; i++){
    sample = (int) Rcpp::runif(1, 0, niter-burnin)(0);
    s_(0) = (S_save.col(sample)(T-1) == 0) ?
    R::rbinom(1, pr_save.slice(sample)(0,0)): R::rbinom(1, pr_save.slice(sample)(1,1));
    if(s_(0) == 0){
      Sig_inv = inv(Sigma0_save.slice(sample)+nugget*eye(p,p));
      mu = mu0_save.col(sample);
      // w.col(i) = inv(Sigma0_save.slice(sample)+nugget*eye(p,p)) * mu0_save.col(sample) / sum(inv(Sigma0_save.slice(sample)+nugget*eye(p,p)) * mu0_save.col(sample));
      w.col(i) = Sig_inv * (m*as_scalar(one.t()*Sig_inv*one)*mu
                              - as_scalar(mu.t()*Sig_inv*one)*mu
                              + as_scalar(mu.t()*Sig_inv*mu)*one
                              - m*as_scalar(mu.t()*Sig_inv*one)*one)
                              / (dot(mu,Sig_inv*mu)*dot(one, Sig_inv*one) - pow(dot(mu, Sig_inv*one),2));
      risk(i) = sqrt(dot(w.col(i), Sigma0_save.slice(sample)*w.col(i)));
    }else{
      // w.col(i) = inv(Sigma1_save.slice(sample)+nugget*eye(p,p)) * mu1_save.col(sample) / sum(inv(Sigma1_save.slice(sample)+nugget*eye(p,p)) * mu1_save.col(sample));
      Sig_inv = inv(Sigma1_save.slice(sample)+nugget*eye(p,p));
      mu = mu1_save.col(sample);
      w.col(i) = Sig_inv * (m*as_scalar(one.t()*Sig_inv*one)*mu
                              -as_scalar(mu.t()*Sig_inv*one)*mu
                              + as_scalar(mu.t()*Sig_inv*mu)*one
                              - m*as_scalar(mu.t()*Sig_inv*one)*one)
                              / (dot(mu,Sig_inv*mu)*dot(one, Sig_inv*one) - pow(dot(mu, Sig_inv*one),2));
      risk(i) = sqrt(dot(w.col(i), Sigma1_save.slice(sample)*w.col(i)));
    }
  }
  return risk;
}

// [[Rcpp::export]]
Rcpp::List gibbs(const uword& niter, const uword& burnin, const mat& y, 
                 const cube& Sigma0, const vec& v0,
                 const mat& mu0, const cube& S0, const int& h, const double nugget, const vec m)
{
  uword T = y.n_rows;
  int p = y.n_cols;
  double init0 = R::runif(0, 1);
  
  mat filter(T, 2, fill::zeros), predict(T, 2, fill::zeros), likelihood(T, 2, fill::zeros), mu(p,2,fill::zeros);
  vec marginal(T, fill::zeros), s(T, fill::zeros), n(4, fill::zeros);
  
  mat mu0_save(p, niter-burnin, fill::zeros), 
  mu1_save(p, niter-burnin, fill::zeros), 
  pr(2, 2, fill::zeros),
  S_save(T, niter-burnin);
  pr(0, 0) = R::rbeta(0, 1);
  pr(0, 1) = 1 - pr(0, 0);
  pr(1, 1) = R::rbeta(0, 1);
  pr(1, 0) = 1 - pr(1, 1);

  cube Sigma0_save(p, p, niter-burnin, fill::zeros), Sigma1_save(p, p, niter-burnin, fill::zeros), 
  filter_save(T, 2, niter-burnin, fill::zeros),
  pr_save(2, 2, niter-burnin, fill::zeros), S(p, p,2,fill::zeros);
  S.slice(0) = S.slice(1) = eye(p, p);
  
  cube y_pred(h, p, niter, fill::zeros);
  
  cube w_save(p, niter, m.n_elem, fill::zeros);
  
  for(uword i=0; i < niter; i++){
    // // step one: forward filter
    forward(filter, predict, likelihood, marginal, y, mu, S, pr, init0);
    // // step two: backward sample
    backward(filter, pr, s, n);
    // // step three: sample transition parameters
    transition(n(0), n(1), n(2), n(3), pr);
    // // step four: sample mean parameters
    location(y, s, mu0, mu, Sigma0, S);
    // // step five: sample scale parameters
    scale(y, s, v0, mu, S0, S);
    // // save MCMC draws after burnin period
    if(i >= burnin){
      filter_save.slice(i-burnin) = filter;
      S_save.col(i-burnin) = s;
      pr_save.slice(i-burnin) = pr;
      mu0_save.col(i-burnin) = mu.col(0);
      mu1_save.col(i-burnin) = mu.col(1);
      Sigma0_save.slice(i-burnin) = S.slice(0);
      Sigma1_save.slice(i-burnin) = S.slice(1);
    }
  }
  // draw from posterior predictive distribution
  y_pred = posterior_predictive_draws(niter, burnin, h,
                                     pr_save, S_save, mu0_save, mu1_save,
                                     Sigma0_save, Sigma1_save);
  mat risk(niter, m.n_elem, fill::zeros);
  for(int i=0; i<m.n_elem; i++){
    w_save.slice(i) = porftfolio_weights(niter, burnin, h, pr_save, S_save, mu0_save, mu1_save, Sigma0_save, Sigma1_save, nugget, m(i));
    risk.col(i) = frontier(niter, burnin, h, pr_save, S_save, mu0_save, mu1_save, Sigma0_save, Sigma1_save, nugget, m(i));
  }
  
  Rcpp::List out(8);
  out["filter"] = filter_save;
  out["mu0_save"] = mu0_save;
  out["mu1_save"] = mu1_save;
  out["Sigma0_save"] = Sigma0_save;
  out["Sigma1_save"] = Sigma1_save;
  out["pr_save"] = pr_save;
  out["S_save"] = S_save;
  out["y_pred"] = y_pred;
  out["pf_weights"] = w_save;
  out["risk"] = risk;
  return(out);
}
```
## Download Data
In this brief case study, I apply my Bayesian Hidden Markov Model (HMM) implementation to an all-ETF portfolio allocation problem by proposing a novel resampled efficient frontier idea based upon the posterior predictive distribution of asset weights, arrived at through mean-variance optimization. The Bayesian HMM resampled frontier framework provides a coherent paradigm for incorporating both parameter estimation uncertainty and market regime uncertainty into the mean-variance optimization process. This is in contrast with the certainty equivalence principal of standard mean-variance optimization, whereby "plug-in" parameter estimates are used in the optimization routine which can lead to badly-leveraged portfolio weights due the inherent unaccounted uncertainties involved in the estimation process [2].

To start, I'll download weekly return data for a basket of five ETFs: 

1. S\&P 500
2. EFA -- a non-US equities fund
3. IJS -- a small-cap value fund
4. EEM -- an emerging-markets fund
5. AGG -- a fixed income bond fund.

```{r warning=F, include=F}
library(quantmod)
library(PerformanceAnalytics)
library(knitr)
library(ggplot2)
library(tidyverse)
library(tidyquant)
library(timetk)
```

```{r ETF, warning=F}
symbols <- c("SPY","EFA", "IJS", "EEM","AGG")

prices <- getSymbols(symbols, src = 'yahoo', from = "2005-01-01",
             auto.assign = TRUE, warnings = FALSE) %>%
  map(~Ad(get(.))) %>%
  reduce(merge) %>%
  `colnames<-`(symbols)

prices_monthly <- to.weekly(prices, indexAt = "last", OHLC = FALSE)
asset_returns_xts <- na.omit(Return.calculate(prices_monthly, method = "log"))
```

I'll visualize the collection of log-returns over time as follows
```{r visual, fig.align="center"}
matplot(asset_returns_xts[,], type="l", ylab="log-return", xlab="Week")
legend(300,.2,legend = symbols[],fill = 1:length(symbols[]),title = "ETFs", cex=.7, horiz = T)
```

To reframe the problem formally, I seek to solve the optimization problem 
$$
\begin{equation}
  \begin{aligned}
  \min_{\omega} & & \omega^\top\Sigma_{T+\tau}\omega \\
  \text{subject to} &  &\omega^\top\mu_{T+\tau}\geq\mu^{\star}\\
  \text{and} & & \omega^\top1  = 1,
  \end{aligned}
\end{equation}
$$
which implies minimizing portfolio variance while ensuring returns of some level $\mu^\star$. I'll employ the posterior predictive distribution of the fitted HMM to obtain a probability distribution of estimates for plausible portfolio weights $\omega$ for the yet unrealized time period $T+\tau$, conditional only upon the return series up to time $T$. Sampling from the posterior predictive distribution is algorithmicly straightforward through Monte Carlo sampling as follows:

1. Draw $\Theta^{(s)}, S_{1:T}^{(s)}\sim p(\Theta, S_{1:T}|r_{1:T})$
2. Draw $S_{T+\tau}^{(s)}\sim p(S_{T+\tau}|S_T^{(s)}, \Theta^{(s)})$
3. Solve (1) to draw from $\omega^{(s)}_{T+\tau}\sim p(\omega_{T+\tau}|r_{1:T},\Theta^{(s)}, S_{T+\tau}^{(s)})$ for a given $\mu^\star$
4. Repeat 1-3 to form a collection of iid random draws from predictive density $p(\omega_{T+\tau}|r_{1:T})$

```{r fit, cache=T}
sampler = function(y, niter, burnin, mu0, Sigma0, v0, S0, m, nugget){
  r <- NULL
  attempt <- 1
  while( is.null(r) && attempt <= 3 ) {
    attempt <- attempt + 1
    try(
       r <- gibbs(niter = niter, burnin = burnin,
                      y = y, Sigma0 = Sigma00, v0 = v0, mu0 = mu0, S0 = S0, h=1, nugget, m)
    )
  }
  return(r)
}
```

```{r include=F}
y = apply(asset_returns_xts, 2, scale, scale=T, center=T)
scales=locs=c()
for(i in 1:ncol(y)){
  scales[i]=attr(scale(asset_returns_xts[,i]), "scaled:scale")
  locs[i]=attr(scale(asset_returns_xts[,i]), "scaled:center")
}
p = ncol(asset_returns_xts)
mu00 = cbind(rep(0, p), rep(0, p))
v00 = c(1,1)
Sigma00 = array(diag(p), dim=c(p,p,2))
S00 = abind::abind(diag(p), diag(p), along = 3)
m = (c(0,.01,.025,.05,.1))
ptm <- Sys.time();
fit = sampler(y, niter=5000, burnin=1000,mu0=mu00,Sigma0=Sigma00,v0=v00,S0=S00,m=m,nugget=0); 
print(Sys.time() - ptm)
```

```{r, echo=F, fig.align="center", fig.show="hold", out.width="60%"}
a=seq(.8,.3,length.out = ncol(y)) 
# par(mar = c(2, 3, 1, 1))
for(i in 1:p){
  rgbcol = col2rgb(1)/255
  hist(fit$pf_weights[i,,1],probability = T, main=symbols[i], xlab="Weight",
       col=rgb(rgbcol[1], rgbcol[2], rgbcol[3], alpha = a[1]),
       ,xlim=c(-5,5), ylim=c(0,4), breaks=100)
  lines(density(fit$pf_weights[i,,1]), col = 1,type="l", lwd=3)
  for(j in 2:length(m)){
    rgbcol = col2rgb(j)/255
    hist(fit$pf_weights[i,,j],probability = T, xlab="Weight",
     col=rgb(rgbcol[1], rgbcol[2], rgbcol[3], alpha = a[j]), breaks=100, add=T)
    lines(density(fit$pf_weights[i,,j]), col = j, type="l", lwd=3)
  }
  legend(2,2,legend = round(m, 5) ,fill = 1:length(m), title = "Return Level", cex=.75)
}

# apply(fit$pr_save, 1:2, mean)
# regimes = apply(fit$S_save, 1, mean)
# par(mar=c(10,5,1,1))
# matplot(regimes,type="l", axes=F, ylab="Regime Probability")
# matplot(1-regimes,type="l", axes=F, add=T, col="darkred")
# axis(2)
# axis(side=1,at=1:nrow(y),labels=rownames(as.data.frame(asset_returns_xts)), las=2)

# kable(cov2cor(apply(fit$Sigma0_save,1:2,median)), digits = 2, caption = "Correlation 1")
# kable(cov2cor(apply(fit$Sigma1_save,1:2,median)), digits = 2, caption = "Correlation 2")
```

Additionally, through this predictive density on $p(\omega_{T+\tau}|r_{1:T})$, I can obtain distributions for perfance metrics such as, e.g., the Sharpe Ratio. Draws from this distribution $p(SR^{(s)}|r_{1:T})$ can be computed through
$$
SR^{(s)}:=\frac{\omega_{T+\tau}^{(s)\top}\mu_{T+\tau}}{\sqrt{\omega_{T+\tau}^{(s)\top}\Sigma_{T+\tau}\omega_{T+\tau}^{(s)}}}.
$$

Finally, the efficient frontier distribution can now be traced out for given values of $\mu^\star$ by then computing the portfolio risk $\sqrt{\omega_{T+\tau}^{(s)\top}\Sigma_{T+\tau}\omega_{T+\tau}^{(s)}}$ for a given sample $\omega_{T+\tau}^{(s)}$ from $p(\omega_{T+\tau}|r_{1:T})$.

```{r}
plot(y=m*scales+locs, x=apply(fit$risk,2,quantile,.5), col=1:length(m), pch=19,cex=2, main="Resampled Frontier")
plot(y=m*scales+locs, x=apply(fit$risk,2,mean), col=1:length(m), pch=17, cex=2, main="Resampled Frontier")
# temp=m*scales+locs
# for(i in 1:ncol(fit$risk)){
#   points(y=rep(temp[i], nrow(fit$risk)), fit$risk[,i], col=i, cex=.1)
# }
# points(y=m*scales+locs, x=apply(fit$risk,2,quantile,.5), col=1:length(m), pch=19,cex=2)
# points(y=m*scales+locs, x=apply(fit$risk,2,quantile,.9), col=1:length(m), pch=19,cex=2)
# lines(y=m*scales+locs, x=apply(fit$risk,2,mean),type="l")

```


# References 
- [*Bayes vs. Resampling: A Rematch*](https://faculty.fuqua.duke.edu/~charvey/Research/Published_Papers/P95_Bayes_vs_Markowitz.pdf) - Harvey, Liechtyb and Liechtyc
- [*Bayesian Methods in Finance*](https://www.wiley.com/en-us/Bayesian+Methods+in+Finance-p-9780470249246) - Rachev, Hsu, Bagasheva, and Fabozzi.
- [*Finite Mixture and Markov Switching Models*](https://link.springer.com/content/pdf/10.1007%2F978-0-387-35768-3.pdf) - Sylvia Fruhwirth-Schnatter.


<!-- https://rviews.rstudio.com/2017/10/11/from-asset-to-portfolio-returns/ -->

<!-- Page 373 in *Finite Mixtures & Markov Swiching Models* to derive posterior predictive density. Then use this and apply page 101 in *Bayesian Methods in Finance* book to construct portfolio and efficient frontier. -->

<!-- To form the posterior predictive distribution of next week's returns, I first work with the one-step-ahead predictive density -- a byproduct of the filter/smoothing updating equations. From this, I'll get the posterior predictive density (unconditional on model parameters) by integrating out $\Theta$ and the latent hidden state $S_{1:T}$. -->

<!-- The posterior predictive density then becomes -->
<!-- $$ -->
<!-- \begin{align} -->
<!-- p(r_{T+1}|r_{1:T}) &= \int p(r_{T+1}|r_{1:T}, \Theta, S_{1:T})d\Theta dS_{1:T}\\ -->
<!-- \end{align} -->
<!-- $$ -->
<!-- The Monte Carlo sampling routine to draw from this distribution is as follows from section 12.4.2: -->

<!-- 1. Draw $\Theta^{(s)}, S_{1:T}^{(s)}\sim p(\Theta, S_{1:T}|r_{1:T})$ -->
<!-- 2. Draw $S_{T+1}\sim p(S_{T+1}|S_T^{(s)}, \Theta^{(s)})$ -->
<!-- 2. Draw $r_{T+1}^{(s)} \sim p(r_{T+1}|r_{1:T}, \Theta^{(s)}, S_{T+1}^{(s)})$ -->
<!-- 3. Repeat 1-3 to form a collection of iid random draws from predictive density $p(r_{T+1}|r_{1:T})$ -->

<!-- # Bayesian Simulation-based Resamped Frontier  -->
<!-- These are early ideas, but consider how this extends the methodology [here](https://faculty.fuqua.duke.edu/~charvey/Research/Published_Papers/P95_Bayes_vs_Markowitz.pdf). -->
<!-- Use the posterior predictive draw algorithm to form efficient frontier resampling. Based on the $S_{>T}$ series, make a draw from the appropriate regime and optimize with respect to those parameters!!! This provides a *Bayesian Resampled Frontier Approach with Hidden Markov Models*. -->

<!-- From this predictive distribution, various optimization routines may be employed for portfolio allocation or counter factual scenarios considered based upon distributional quantiles. For example, say I have the posterior predictive distribution for $p(r_{T+1}|r_{1:T}$. Then following classical mean-variance optimization, the allocator seeks to solve the optimization problem -->


<!-- ```{r backtest, include=F, eval=F} -->
<!-- start = which(format(as.Date(rownames(as.data.frame(asset_returns_xts))), "%Y")=="2010")[1] -->
<!-- forecast = nrow(y)-start -->
<!-- y_walking = y[1:start,] -->
<!-- weights = matrix(0, nrow=ncol(y), ncol = nrow(y)) -->
<!-- m = .05 -->
<!-- for(j in 1:forecast){ -->
<!--   fit = sampler(y_walking) -->
<!--   # determine current regime -->
<!--   regimes = apply(fit$filter, 1:2, mean) -->
<!--   current_regime = which.max(regimes[nrow(regimes),]) -->

<!--   if(current_regime==1){ -->
<!--     mu = colMeans(fit$mu0_save) -->
<!--     Sigma_t = apply(fit$Sigma0_save,1:2,mean) -->
<!--   }else{ -->
<!--     mu = colMeans(fit$mu1_save) -->
<!--     Sigma_t = apply(fit$Sigma1_save,1:2,mean) -->
<!--   } -->
<!--   w = solve(Sigma_t) %*% (m*) -->
<!--   # calculate weights based on current regime -->

<!--   # calculate returns based on weights -->
<!--   returns[j] = w %*% t(monthly[months[j],]) -->

<!--   # save weight trajectories -->
<!--   weights[,j] = w -->
<!--   # jump one month ahead and refit HMM -->
<!--   y_walking  = y[1:(start+j)] -->
<!-- } -->

<!-- y = apply(asset_returns_xts, 2, scale) -->
<!-- { -->
<!-- ptm <- Sys.time(); fit = sampler(y); print(Sys.time() - ptm) -->
<!-- regimes = apply(fit$filter, 1:2, mean) -->
<!-- par(mar=c(10,5,1,1)) -->
<!-- matplot(regimes,type="l", axes=F) -->
<!-- axis(2) -->
<!-- axis(side=1,at=1:nrow(y),labels=rownames(as.data.frame(asset_returns_xts)), las=2) -->
<!-- } -->
<!-- plot(apply(regimes,1,which.max), type="l") -->


<!-- rowMeans(fit$mu0_save) -->
<!-- rowMeans(fit$mu1_save) -->

<!-- matplot(apply(fit$filter, 1:2, median), type="l") -->

<!-- heatmap(cov2cor(apply(fit$Sigma1_save,1:2,mean)), Rowv = NA, Colv = NA) -->
<!-- heatmap(cov2cor(apply(fit$Sigma0_save,1:2,mean)), Rowv = NA, Colv = NA) -->

<!-- for(i in 1:p){ -->
<!--   hist(fit$mu0_save[i,], breaks=100, xlim=c(-5,5), col="darkred") -->
<!--   hist(fit$mu1_save[i,], breaks=100, xlim=c(-5,5), col="darkblue", add=T) -->
<!-- } -->
<!-- plot(rowMeans(apply(fit$mu0_save, 1:2, mean)), pch=19, col="darkblue", ylim = c(-4,4)) -->
<!-- points(apply(fit$mu0_save,1, quantile, .975), pch="-", col="darkblue") -->
<!-- points(apply(fit$mu0_save,1, quantile, .025), pch="-", col="darkblue") -->

<!-- points(rowMeans(apply(fit$mu1_save, 1:2, mean)), pch=19, col="darkred", ylim = c(-4,4)) -->
<!-- points(apply(fit$mu1_save,1, quantile, .975), pch="-", col="darkred") -->
<!-- points(apply(fit$mu1_save,1, quantile, .025), pch="-", col="darkred") -->
<!-- ``` -->
